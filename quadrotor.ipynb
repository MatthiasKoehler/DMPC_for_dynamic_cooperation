{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Synchronization and flocking\n",
    "\n",
    "This notebook sets up the simulation for the example in Section VI.B of\n",
    "\n",
    "> Distributed Model Predictive Control for Dynamic Cooperation of Multi-Agent Systems --- Matthias Köhler, Matthias A. Müller, and Frank Allgöwer\n",
    "\n",
    "If turned on below, the simulation data is saved to a data file in the folder `./data/`.\n",
    "This is recommended if the exported python file is run in order to access the simulation data later.\n",
    "The data can be visualised using the accompanying notebook `quadrotor_evaluation.ipynb`.\n",
    "\n",
    "The simulation data used in the paper is contained in the file `./data/quadrotor_data.dill`. \n",
    "This data is animated in `quadrotor.mp4` and `quadrotor_3D.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import casadi as cas\n",
    "import dill\n",
    "from datetime import datetime\n",
    "import auxiliaries as aux\n",
    "import time\n",
    "import cvxpy\n",
    "\n",
    "print(f'CVXPY recognizes the following solvers: {cvxpy.installed_solvers()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main settings\"\"\"\n",
    "start_time = time.time()  # Time the total execution of the script.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Data saving on hard drive to './data/'.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "save_data = True  # Whether to save the simulation data to a file.\n",
    "save_interval_steps = 10  # Save data every 'save_interval_steps' steps. If 0, no continuous saving is done.\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# MAS parameters\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "MAS_type = 'quadrotor10'\n",
    "N = 10                              # Set the prediction horizon used in the MPC optimization problem.\n",
    "h = 0.1                             # Set the step size of the discretization of the continuous-time dynamics.\n",
    "num_agents = 4                      # Set number of quadrotors.\n",
    "load_terminal_ingredients = False   # If False, compute terminal ingredients. If True, load them from a file.\n",
    "save_terminal_ingredients = False   # If True, save terminal ingredients to a file.\n",
    "method = 'Euler'                    # Discretisation method: 'Euler', 'RK4', 'RK2'\n",
    "distance = 0.4                      # Set the minimum distance between quadrotors.       \n",
    "N_2nd_phase = 30                    # Set the prediction horizon used in the MPC optimization problem for the second phase. \n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Simulation parameters.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "max_sim_time = 700                  # Set the maximum simulation time step.\n",
    "terminal_ingredients_type = 'set'   # Choice between 'set', 'equality', and 'without'.\n",
    "cutoff_treshold = -1e-6             # Stop the simulation if the value function falls below this threshold.\n",
    "average_treshold = -1e-6            # Stop the simulation if the standard deviation of the value function falls below this threshold.\n",
    "max_iter = None                     # Maximum number of iterations for ipopt. None allows ipopt's default.\n",
    "\n",
    "sqp_max_iter = 10                   # Number of SQP iterations in the first phase.\n",
    "sqp_max_iter_2nd_phase = 14         # Number of SQP iterations in the second phase.\n",
    "\n",
    "admm_max_iter = 55                  # Number of ADMM iterations to solve the QP in each SQP iteration in the first phase.\n",
    "admm_max_iter_2nd_phase = 100       # Number of ADMM iterations to solve the QP in each SQP iteration in the second phase.\n",
    "admm_penalty = 25                   # Penalty parameter for the ADMM algorithm in the first phase.\n",
    "admm_penalty_2nd_phase = 25         # Penalty parameter for the ADMM algorithm in the second phase.\n",
    "\n",
    "solver = 'gurobi'                   # Solver for local QPs, e.g. 'osqp', 'qpoases', 'gurobi', 'ipopt'\n",
    "parallel = True                     # Whether to use parallelization for the local QPs.\n",
    "\n",
    "switching_time = 350                # Time step when the switching between the two phases of the cooperative task occurs.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Cooperative task.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "T = 50                              # Periodicity of the cooperative task.\n",
    "T_2nd_phase = 1                     # Periodicity of the cooperative task in the second phase (should be set to 1).\n",
    "\n",
    "coop_task = 'circle'\n",
    "\n",
    "print(f\"Cooperative task is '{coop_task}'.\")\n",
    "print(f\"Last simulation time is {max_sim_time*h} s (~ {max_sim_time*h // 60} min) with {max_sim_time} simulation steps.\")\n",
    "print(f\"Period length is {T*h} s (~ {T*h // 60} min) with {T} simulation steps.\")\n",
    "print(f\"Prediction horizon is {N*h} s (~ {N*h // 60} min) with {N} prediction steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialise data saving.\"\"\"\n",
    "data = {}\n",
    "data['cooperative_task'] = {}\n",
    "data['cooperative_task']['type'] = coop_task\n",
    "data['MAS_parameters'] = {}\n",
    "data['MAS_parameters']['num_agents'] = num_agents\n",
    "data['MAS_parameters']['MAS_type'] = MAS_type\n",
    "data['MAS_parameters']['h'] = h\n",
    "data['sim_data'] = {'max_sim_time': max_sim_time}\n",
    "data['sim_pars'] = {'N': N,\n",
    "                    'cutoff_threshold': cutoff_treshold,\n",
    "                    'average_treshold': average_treshold,\n",
    "                    'terminal_ingredients_type': terminal_ingredients_type,\n",
    "                    'max_iter': max_iter,\n",
    "                    'T': T,\n",
    "                    'sqp_max_iter': sqp_max_iter,\n",
    "                    'admm_max_iter': admm_max_iter, \n",
    "                    'admm_penalty': admm_penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-agent system\"\"\"\n",
    "agents = aux.get_quadrotor10_MAS(data)\n",
    "positions = [\n",
    "    # np.vstack([ -2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    np.vstack([ -1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    np.vstack([ -0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    np.vstack([  0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    np.vstack([  1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    np.vstack([  2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    ]\n",
    "data['sim_pars']['positions'] = positions\n",
    "# Define an all-to-all topology.\n",
    "for i, agent in enumerate(agents):\n",
    "    agent.neighbours = []\n",
    "    for j in range(len(agents)):\n",
    "        if j != i:\n",
    "            agent.neighbours.append(agents[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cooperative tasks\"\"\"\n",
    "## Define the task to following a circle trajectory with a shifted phase:\n",
    "phase_shift = 45*np.pi/180  # Set a phase shift between agents.\n",
    "radius_lb = 1.  # Set a lower bound for the radius.\n",
    "radius_ub = 2.0   # Set an upper bound for the radius.\n",
    "radius_softplus_weight = 1.  # Set a weight for the softplus function if the radius is penalized using softplus.\n",
    "radius_softplus_decay = 100.  # Set a decay for the softplus function, i.e. a multiplicative factor for the exponent of the exponential function.\n",
    "constrain_radius = True  # Decide whether to use a hard (True) or soft constraint (False) to constrain the radius.  \n",
    "\n",
    "for agent in agents[1:]:\n",
    "    agent.cooperation_neighbours = [agents[0]]  # Set the neighbours for the flocking task.\n",
    "agents[0].cooperation_neighbours = agents[1:] \n",
    "\n",
    "def get_leader_reference(t, switching_time):\n",
    "    rate = switching_time\n",
    "    \n",
    "    k = t - switching_time\n",
    "    \n",
    "    ref = [0., 0., 0.]\n",
    "    \n",
    "    if k <= rate:\n",
    "        ref[0] = -10. + (k/rate)*20.\n",
    "        ref[1] = -10. + (k/rate)*20.\n",
    "    elif k <= 2*rate:\n",
    "        ref[0] =  10. - ((k - rate)/rate)*20.\n",
    "        ref[1] =  10. - ((k - rate)/rate)*20.\n",
    "\n",
    "    return np.vstack(ref)\n",
    "\n",
    "data['MAS_parameters']['collision_distance'] = distance\n",
    "coop_kwargs = {\n",
    "    't': 0, \n",
    "    'agents': agents, \n",
    "    'T': T, \n",
    "    'switching_time': switching_time, \n",
    "    'phase_shift': phase_shift, \n",
    "    'radius_lb': radius_lb, \n",
    "    'radius_ub': radius_ub, \n",
    "    'radius_softplus_weight': radius_softplus_weight,\n",
    "    'radius_softplus_decay': radius_softplus_decay, \n",
    "    'constrain_radius': constrain_radius, \n",
    "    'get_leader_reference':get_leader_reference,\n",
    "    'distance': distance}\n",
    "\n",
    "coop_task_builder = aux.set_cooperative_task_to_circle\n",
    "if 'cooperative_task' not in data:\n",
    "    data['cooperative_task'] = {}\n",
    "data['cooperative_task']['T'] = T\n",
    "data['cooperative_task']['switching_time'] = switching_time\n",
    "data['cooperative_task']['phase_shift'] = phase_shift\n",
    "data['cooperative_task']['radius_lower_bound'] = radius_lb\n",
    "data['cooperative_task']['radius_upper_bound'] = radius_ub\n",
    "data['cooperative_task']['radius_constrained'] = constrain_radius\n",
    "data['cooperative_task']['kwargs'] = coop_kwargs\n",
    "if not constrain_radius:\n",
    "    data['cooperative_task']['radius_softplus_weight'] = radius_softplus_weight\n",
    "    data['cooperative_task']['radius_softplus_decay'] = radius_softplus_decay\n",
    "    \n",
    "# Call the task builder to establish constraints.\n",
    "aux.set_cooperative_task_to_circle(**coop_kwargs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Terminal ingredients\"\"\"\n",
    "if load_terminal_ingredients: \n",
    "    aux.load_generic_terminal_ingredients(agents[0], './data/quadrotor_terminal_ingredients')\n",
    "    for agent in agents[1:]:\n",
    "        agent.terminal_ingredients = agents[0].terminal_ingredients\n",
    "    print(f'Loaded terminal ingredients with the following set sizes:')\n",
    "    for agent in agents:\n",
    "        print(f'A{agent.id}: {agent.terminal_ingredients['size']}')\n",
    "else:    \n",
    "    aux.compute_terminal_ingredients_for_quadrotor(\n",
    "        agent=agents[0], \n",
    "        data=data, \n",
    "        grid_resolution=1000, \n",
    "        num_decrease_samples=1000, \n",
    "        alpha = 0.5,\n",
    "        alpha_tol = 1e-9,\n",
    "        references_are_equilibria=False,\n",
    "        compute_size_for_decrease=True,\n",
    "        compute_size_for_constraints=True,\n",
    "        epsilon=1e-2,\n",
    "        verbose=2,\n",
    "        solver='MOSEK')\n",
    "    for agent in agents[1:]:\n",
    "        agent.terminal_ingredients = agents[0].terminal_ingredients\n",
    "if save_terminal_ingredients:\n",
    "    aux.save_generic_terminal_ingredients(agents[0], './data/quadrotor_terminal_ingredients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simulation run\"\"\"\n",
    "# Initalization:\n",
    "positions = data['sim_pars']['positions']\n",
    "for idx, agent in enumerate(agents):\n",
    "    agent.current_state = np.vstack([positions[idx]])\n",
    "\n",
    "# Initialize data tracking.\n",
    "data['sim_data']['yT'] = {}  # Track the cooperation outputs.\n",
    "data['sim_data']['xT'] = {}  # Track the cooperation state trajectory.\n",
    "data['sim_data']['uT'] = {}  # Track the cooperation input trajectory.\n",
    "data['sim_data']['x'] = {}  # Track the open-loop state prediction.\n",
    "data['sim_data']['u'] = {}  # Track the open-loop input prediction.\n",
    "data['sim_data']['tracking_cost'] = []  # Track the value of the tracking part.\n",
    "data['sim_data']['cooperative_cost'] = []  # Track the value of the cooperation objective function part.\n",
    "data['sim_data']['change_cost'] = []  # Track the value of the penalty on the change of the cooperation output part.\n",
    "data['sim_data']['J'] = []  # Track the value of the cost.\n",
    "    \n",
    "for agent in agents:\n",
    "    data['sim_data']['xT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['yT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['uT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['x'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['u'][f'A{agent.id}'] = []\n",
    "    \n",
    "# Initialize the penalty weight for the change in the cooperation output.\n",
    "for agent in agents:\n",
    "    # Initialize an empty previously optimal cooperation output for each agent.\n",
    "    agent.yT_pre = None\n",
    "    agent.MPC_sol = None\n",
    "    \n",
    "    agent.penalty_weight = 1e-4/T  # Set the weight of the penalty on the change in the cooperation output.\n",
    "    data['agents'][f'A{agent.id}']['penalty_weight'] = agent.penalty_weight\n",
    "\n",
    "# Build the closed-loop state evolution of each agent and save it as an attribute of the agent.\n",
    "for agent in agents:\n",
    "    agent.cl_x = [agent.current_state.copy()]\n",
    "    agent.cl_u = []\n",
    "    \n",
    "# Initialize a filestamp if continuous saving is activated.\n",
    "if save_data and save_interval_steps > 0:\n",
    "    filestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filestamp = f'{filestamp}_quicksave_{coop_task}'\n",
    "\n",
    "for t in range(0, max_sim_time+1):    \n",
    "    print(f\"{t}: -------------------------------------------------------------------------\")  # Print the time step:\n",
    "    if t == 0:\n",
    "        warm_start = aux.quadrotor_warm_start_at_t0(agents, N, T)\n",
    "    elif t == switching_time:\n",
    "        # Pop outdated multipliers.\n",
    "        for agent in agents:\n",
    "            agent.MPC_sol.pop(f'A{agent.id}_ineq_mult')\n",
    "            agent.MPC_sol.pop(f'A{agent.id}_consensus_mult')\n",
    "            if N_2nd_phase != N or T_2nd_phase != T:\n",
    "                agent.MPC_sol.pop(f'A{agent.id}_eq_mult')\n",
    "            \n",
    "        # Compute a new warm start.\n",
    "        warm_start = aux.quadrotor_warm_start_at_switching_time(agents, N_2nd_phase, T_2nd_phase, N, T, terminal_ingredients_type)\n",
    "                \n",
    "        # Update the preediction horizon, and the periodicity.\n",
    "        N = N_2nd_phase\n",
    "        T = T_2nd_phase\n",
    "        coop_kwargs['T'] = T\n",
    "        \n",
    "        # Update the iteration numbers for the second phase.\n",
    "        admm_max_iter = admm_max_iter_2nd_phase \n",
    "        data['sim_pars']['admm_max_iter_2nd_phase'] = admm_max_iter_2nd_phase\n",
    "        sqp_max_iter = sqp_max_iter_2nd_phase\n",
    "        data['sim_pars']['sqp_max_iter_2nd_phase'] = sqp_max_iter_2nd_phase\n",
    "        admm_penalty = admm_penalty_2nd_phase\n",
    "        data['sim_pars']['admm_penalty_2nd_phase'] = admm_penalty_2nd_phase\n",
    "\n",
    "    else:\n",
    "        warm_start = aux.compute_decentralized_following_warm_start_dynamic_cooperative_DMPC(agents, T, N, terminal_ingredients_type=terminal_ingredients_type)\n",
    "\n",
    "    coop_kwargs['t'] = t\n",
    "    coop_kwargs['agents'] = agents\n",
    "    # Generate and solve the optimization problem for MPC for dynamic cooperation.\n",
    "    res = aux.solve_MPC_for_dynamic_cooperation_decentrally(\n",
    "        sqp_max_iter, \n",
    "        admm_max_iter, \n",
    "        admm_penalty,\n",
    "        t, \n",
    "        agents, \n",
    "        N=N, \n",
    "        T=T,\n",
    "        warm_start=warm_start, \n",
    "        solver=solver,\n",
    "        terminal_ingredients_type=terminal_ingredients_type,\n",
    "        coop_task_builder=coop_task_builder, \n",
    "        coop_kwargs=coop_kwargs,\n",
    "        max_iter=max_iter,\n",
    "        verbose=2,\n",
    "        parallel=parallel\n",
    "    )\n",
    "    print(f'Solved at time step {t} with f* = {float(res[\"J\"]):.5e}')\n",
    "    \n",
    "    data['sim_data']['tracking_cost'].append(res['tracking_cost'])\n",
    "    data['sim_data']['cooperative_cost'].append(res['cooperative_cost'])\n",
    "    data['sim_data']['change_cost'].append(res['change_cost']) \n",
    "    data['sim_data']['J'].append(res['J']) \n",
    "    \n",
    "    for agent in agents:\n",
    "        # # Keep track of open-loop solutions, reshaped ready for plotting.\n",
    "        data['sim_data']['yT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_yT'].reshape(T, agent.output_dim).T)\n",
    "        data['sim_data']['xT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_xT'].reshape(T, agent.state_dim).T)\n",
    "        data['sim_data']['uT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_uT'].reshape(T, agent.input_dim).T)\n",
    "        data['sim_data']['u'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_u'].reshape(N, agent.input_dim).T)\n",
    "        # The prediction starts with x(1|t), hence x(0|t) = x(t) needs to be prepended.\n",
    "        data['sim_data']['x'][f'A{agent.id}'].append(np.hstack([np.array(agent.current_state), agent.MPC_sol[f'A{agent.id}_x'].reshape(N, agent.state_dim).T]))\n",
    "\n",
    "        # Update the current state of the agents.\n",
    "        agent.current_state = agent.dynamics(x=agent.current_state, u=agent.MPC_sol[f'A{agent.id}_u'][0:agent.input_dim])['x+']\n",
    "        agent.cl_x.append(np.array(agent.current_state))  # Keep track of the current state.\n",
    "        agent.cl_u.append(np.array(agent.MPC_sol[f'A{agent.id}_u'][0:agent.input_dim]))  # Keep track of the current input.\n",
    "        \n",
    "        # Set the previously optimal trajectory:\n",
    "        agent.yT_pre = np.vstack([agent.MPC_sol[f'A{agent.id}_yT'][agent.output_dim :], agent.MPC_sol[f'A{agent.id}_yT'][0 : agent.output_dim]])\n",
    "        \n",
    "    # Stop the simulation if the cost falls below a threshold.\n",
    "    if res['J'] <= cutoff_treshold:\n",
    "        print(f\"The value function has fallen below {cutoff_treshold} at time step {t}.\")\n",
    "        data['sim_data']['max_sim_time'] = t\n",
    "        break\n",
    "    # Stop the simulation if the cost has converged; i.e. the standard deviation over a window has fallen below a threshold.\n",
    "    if t > 10 and np.std(data['sim_data']['J'][t-10:t]) <= average_treshold:\n",
    "        print(f\"The standard deviation of the value function has fallen below {average_treshold} at time step {t}.\")\n",
    "        data['sim_data']['max_sim_time'] = t\n",
    "        break\n",
    "    # Save the data after each specified time step.\n",
    "    if save_data and t > 0 and save_interval_steps > 0 and t % save_interval_steps == 0:\n",
    "        aux.save_data(data, agents, filestamp)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "print(f\"\\nTotal runtime: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Colour palette\"\"\"\n",
    "colours = [\n",
    "    \"#0072B2\",  # blue\n",
    "    \"#D55E00\",  # orange\n",
    "    \"#009E73\",  # green\n",
    "    \"#CC79A7\",  # magenta\n",
    "    \"#56B4E9\",  # light blue\n",
    "    \"#E69F00\",  # yellow-orange\n",
    "    \"#B22222\",  # red\n",
    "    \"#6A3D9A\",  # purple\n",
    "    \"#117733\",  # teal green\n",
    "    \"#88CCEE\",  # cyan\n",
    "    \"#DDCC77\",  # muted yellow-orange\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transform data.\"\"\"\n",
    "# Transform the costs into numpy arrays.\n",
    "data['sim_data']['cooperative_cost'] = np.vstack(data['sim_data']['cooperative_cost']).flatten()\n",
    "data['sim_data']['tracking_cost'] = np.vstack(data['sim_data']['tracking_cost']).flatten()\n",
    "data['sim_data']['change_cost'] = np.vstack(data['sim_data']['change_cost']).flatten()\n",
    "data['sim_data']['J'] = np.vstack(data['sim_data']['J']).flatten()\n",
    "\n",
    "# Extract some parameters.\n",
    "max_sim_time = data['sim_data']['max_sim_time']\n",
    "\n",
    "# Transform the tracked closed-loop trajectories of each agent into a matrix.\n",
    "for agent in agents:\n",
    "    if type(agent.cl_x) == list:\n",
    "        agent.cl_x = np.hstack(agent.cl_x)\n",
    "        agent.cl_u = np.hstack(agent.cl_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save data\"\"\"\n",
    "if save_data:\n",
    "    aux.save_data(data, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Value function.\"\"\"\n",
    "# Plot from t1 to t2.\n",
    "t1 = 0\n",
    "t2 = max_sim_time+1\n",
    "\n",
    "# Select a feasible start time (the end time is controlled below).\n",
    "t1 = min(t1, max_sim_time+1)\n",
    "\n",
    "# Draw the evolution in state space:\n",
    "fig_V, ax_V = plt.subplots(figsize=(10, 6), num='state evolution')\n",
    "\n",
    "stop_time = data['sim_data']['cooperative_cost'][t1:t2].shape[0]\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['cooperative_cost'][t1:t2], label='cooperative', color=colours[0])\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['tracking_cost'][t1:t2], label='tracking', color=colours[1])\n",
    "ax_V.plot(range(max(t1,1), min(t2, stop_time)), data['sim_data']['change_cost'][max(t1,1):t2], label='change', color=colours[2])\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['J'][t1:t2], '--', label='J', color=colours[3])\n",
    "    \n",
    "ax_V.set_xlabel('time steps')\n",
    "ax_V.set_title(f'Value function over time')\n",
    "ax_V.legend()\n",
    "ax_V.grid()\n",
    "\n",
    "# ax_V.set_yscale('log')  # Set the y-axis to logarithmic scale.\n",
    "\n",
    "print(f'Value function difference between the first and last time step: {data[\"sim_data\"][\"J\"][-1] - data[\"sim_data\"][\"J\"][0]}')\n",
    "print(f'Value function at start: {data[\"sim_data\"][\"J\"][0]:15.4e}')\n",
    "print(f'Value function at stop : {data[\"sim_data\"][\"J\"][-1]:15.4e}; diff: {data[\"sim_data\"][\"J\"][-1] - data[\"sim_data\"][\"J\"][0]:15.4e}')\n",
    "print(f'Cooperation cost at start : {data[\"sim_data\"][\"cooperative_cost\"][0]:15.4e}')\n",
    "print(f'Cooperation cost at stop : {data[\"sim_data\"][\"cooperative_cost\"][-1]:15.4e}; diff: {data[\"sim_data\"][\"cooperative_cost\"][-1] - data[\"sim_data\"][\"cooperative_cost\"][0]:15.4e}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2D position\"\"\"\n",
    "# Plot from t1 to t2.\n",
    "t1 = 0\n",
    "t2 = max_sim_time + 1\n",
    "step = 1\n",
    "\n",
    "# Select a feasible start time (the end time is controlled automatically).\n",
    "t1 = min(t1, max_sim_time+1)\n",
    "\n",
    "# Draw the evolution in state space:\n",
    "fig_cl, ax_cl = plt.subplots(figsize=(10, 10), num='state evolution')\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    cl_x = agent.cl_x\n",
    "    ax_cl.plot(cl_x[0, t1 : t2+1:step], cl_x[1,t1 : t2+1:step], color=colours[i], label=f'A{agent.id}_x', \n",
    "               #marker='o', markersize=2, \n",
    "               linewidth=1.5)\n",
    "    # Mark the initial state with a larger circle.\n",
    "    ax_cl.plot(cl_x[0,t1], cl_x[1,t1], color=colours[i], marker='o', markersize=6)\n",
    "    # Mark the final state with a cross.\n",
    "    ax_cl.plot(cl_x[0,t2], cl_x[1,t2], color=colours[i], marker='x', markersize=6)\n",
    "    \n",
    "    ax_cl.plot(data['sim_data']['yT'][f'A{agent.id}'][-1][0, :], data['sim_data']['yT'][f'A{agent.id}'][-1][1, :], color=colours[i], markersize=2, linewidth=1, marker='o', label=f'A{agent.id}_yT', alpha=0.25)\n",
    "\n",
    "ax_cl.set_xlabel('$x_1$')    \n",
    "ax_cl.set_ylabel('$x_2$')\n",
    "ax_cl.set_title(f'Closed-loop position from $t = {t1}$ to $t = {t2}$ with step {step}')\n",
    "ax_cl.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collision\"\"\"\n",
    "\n",
    "# Plot from t1 to t2.\n",
    "t1 = 0\n",
    "t2 = max_sim_time + 1\n",
    "\n",
    "fig_d, ax_d = plt.subplots(figsize=(6, 6))\n",
    "ax_d.set_xlabel('$t$') \n",
    "ax_d.set_ylabel(f'distance')\n",
    "ax_d.set_title(f'Distance in position from t = {t1} to t = {t2}')\n",
    "ax_d.grid(True)\n",
    "\n",
    "considered_pairs = {}\n",
    "for i, agent in enumerate(agents):\n",
    "    a1 = agent\n",
    "    for neighbour in agent.neighbours:\n",
    "        a2 = neighbour\n",
    "        if (a2.id, a1.id) in considered_pairs:\n",
    "            continue\n",
    "        else:\n",
    "            considered_pairs[(a1.id, a2.id)] = True\n",
    "\n",
    "    distances = []\n",
    "    for t in range(t1, t2+1):\n",
    "        distances.append(np.linalg.norm(a1.cl_x[0:2, t] - a2.cl_x[0:2, t])) \n",
    "\n",
    "    ax_d.plot(range(t1, t2+1), distances, color=colours[i], label=f'||A{a1.id}_x[0:2] - A{a2.id}_x[0:2]||', markersize=0, linewidth=2, marker='o')\n",
    "ax_d.plot(range(t1, t2+1), [data['MAS_parameters']['collision_distance']]*len(range(t1, t2+1)), color='black', label=f'boundary', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
