{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Satellite constellation\n",
    "\n",
    "This notebook sets up the simulation for the example in Section IV.B of\n",
    "\n",
    "> Distributed Model Predictive Control for Dynamic Cooperation of Multi-Agent Systems --- Matthias Köhler, Matthias A. Müller, and Frank Allgöwer\n",
    "\n",
    "If the flag below is set to True, the simulation data is saved to a data file in the folder `./data/`.\n",
    "This is recommended if the exported python file is run in order to access the simulation data later.\n",
    "\n",
    "The data can be visualised using the accompanying notebook `satellite_constellation_evaluation.ipynb`.\n",
    "\n",
    "The simulation data used in the paper is contained in the file `./data/satellite_constellation_data.dill`. \n",
    "This data is animated in `satellite_constellation.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import casadi as cas\n",
    "import dill\n",
    "from datetime import datetime\n",
    "import auxiliaries as aux\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main settings\"\"\"\n",
    "start_time = time.time()  # Time the total execution of the script.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Data saving on hard drive to './data/'.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "save_data = True  # Whether to save the simulation data to a file.\n",
    "save_interval_steps = 10  # Save data every 'save_interval_steps' steps. If 0, no continuous saving is done.\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# MAS parameters\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "MAS_type = 'satellites'\n",
    "N = 3*47                            # Set the prediction horizon used in the MPC optimization problem.\n",
    "h = 120                             # Set the step size of the discretization of the continuous-time dynamics.\n",
    "num_agents = 5                      # Set number of agents.\n",
    "scaling = 'Mm'                      # Scaling for the satellite example, i.e. 'm', 'km', 'Mm'.\n",
    "method = 'RK4'                      # Discretisation method: 'Euler', 'RK4', 'RK2' (where applicable)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Simulation parameters.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "max_sim_time = 2000                     # Set the last simulation time step.\n",
    "terminal_ingredients_type = 'equality'  # Choose terminal equality constraints.\n",
    "cutoff_treshold = -1e-6                 # Stop the simulation if the value function falls below this threshold.\n",
    "average_treshold = -1e-6                # Stop the simulation if the standard deviation of the value function falls below this threshold.\n",
    "\n",
    "max_iter = None                         # Maximum number of iterations for ipopt. None allows ipopt's default.\n",
    "\n",
    "sqp_max_iter = 5                        # Number of SQP iterations.\n",
    "admm_max_iter = 40                      # Number of ADMM iterations to solve the QP in each SQP iteration.\n",
    "admm_penalty = 2                        # Penalty parameter for ADMM.\n",
    "\n",
    "solver = 'gurobi'                       # Solver for local QPs, e.g. 'osqp', 'qpoases', 'gurobi', 'ipopt'\n",
    "parallel = True                         # Whether to use parallelization for the local QPs.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Cooperative task.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "T = 47                                  # Set the period length of the cooperative task.\n",
    "theta_des = 45                          # Desired angle for the constellation task in degrees.     \n",
    "deorbit_time = 750                      # Time step at which two satellites are removed from the constellation.\n",
    "\n",
    "coop_task = 'constellation'\n",
    "\n",
    "print(f\"Last simulation time is {max_sim_time*h} s (~ {max_sim_time*h // 60} min) with {max_sim_time} simulation steps.\")\n",
    "print(f\"Period length is {T*h} s (~ {T*h // 60} min) with {T} simulation steps.\")\n",
    "print(f\"Prediction horizon is {N*h} s (~ {N*h // 60} min) with {N} prediction steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialise data saving.\"\"\"\n",
    "data = {}\n",
    "data['cooperative_task'] = {}\n",
    "data['cooperative_task']['type'] = coop_task\n",
    "data['MAS_parameters'] = {}\n",
    "data['MAS_parameters']['num_agents'] = num_agents\n",
    "data['MAS_parameters']['MAS_type'] = MAS_type\n",
    "data['MAS_parameters']['h'] = h\n",
    "data['sim_data'] = {'max_sim_time': max_sim_time}\n",
    "data['sim_pars'] = {'N': N,\n",
    "                    'cutoff_threshold': cutoff_treshold,\n",
    "                    'average_treshold': average_treshold,\n",
    "                    'terminal_ingredients_type': terminal_ingredients_type,\n",
    "                    'max_iter': max_iter,\n",
    "                    'T': T,\n",
    "                    'sqp_max_iter': sqp_max_iter,\n",
    "                    'admm_max_iter': admm_max_iter, \n",
    "                    'admm_penalty': admm_penalty,\n",
    "                    'solver': solver}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-agent system\"\"\"\n",
    "agents = aux.get_satellites_MAS(data, scaling, method)\n",
    "# Define the initial conditions.\n",
    "ics = []\n",
    "sf = data['MAS_parameters']['scaling_factor']\n",
    "for i, agent in enumerate(agents):\n",
    "    ics.append(np.array([[0.0], [np.radians(i*25)], [0.], [np.sqrt(agent.mu/(agent.r0)**3)]]))\n",
    "# Set the initial condition.\n",
    "for idx, agent in enumerate(agents):\n",
    "    agent.current_state = ics[idx]\n",
    "    print(f'Orbital radius for agent {agent.id}: {agent.r0 + agent.current_state[0,0]:.6e} {agent.scaling} (r0 + {agent.current_state[0,0]:.1e} {agent.scaling}) with periodicity {T}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cooperative tasks\"\"\"\n",
    "## Define the task of agreeing on an orbit and having a fixed distance between agents.\n",
    "\n",
    "if len(agents) > len(ics):\n",
    "    raise ValueError(f'The number of agents ({len(agents)}) exceeds the number of initial positions ({len(ics)}) of the {coop_task} task!')\n",
    "\n",
    "data['positions'] = ics\n",
    "    \n",
    "coop_task_builder = aux.set_cooperative_task_to_constellation\n",
    "deorbited_satellites = []\n",
    "coop_kwargs={'t': 0, 'agents': agents, 'weight': 0.5, 'N': N, 'T': T, 'theta_des': theta_des, 'sf': data['MAS_parameters']['scaling_factor']}\n",
    "data['cooperative_task']['T'] = T\n",
    "data['cooperative_task']['kwargs'] = coop_kwargs\n",
    "data['cooperative_task']['theta_des'] = theta_des\n",
    "\n",
    "# Call the task builder to establish constraints.\n",
    "aux.set_cooperative_task_to_constellation(**coop_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simulation run\"\"\"\n",
    "# Initalization:\n",
    "for idx, agent in enumerate(agents):\n",
    "    agent.current_state = ics[idx]\n",
    "\n",
    "# Initialize data tracking.\n",
    "data['sim_data']['yT'] = {}  # Track the cooperation outputs.\n",
    "data['sim_data']['xT'] = {}  # Track the cooperation state trajectory.\n",
    "data['sim_data']['uT'] = {}  # Track the cooperation input trajectory.\n",
    "data['sim_data']['x'] = {}  # Track the open-loop state prediction.\n",
    "data['sim_data']['u'] = {}  # Track the open-loop input prediction.\n",
    "data['sim_data']['tracking_cost'] = []  # Track the value of the tracking part.\n",
    "data['sim_data']['cooperative_cost'] = []  # Track the value of the cooperation objective function part.\n",
    "data['sim_data']['change_cost'] = []  # Track the value of the penalty on the change of the cooperation output part.\n",
    "data['sim_data']['J'] = []  # Track the value of the cost.\n",
    "    \n",
    "for agent in agents:\n",
    "    data['sim_data']['xT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['yT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['uT'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['x'][f'A{agent.id}'] = []\n",
    "    data['sim_data']['u'][f'A{agent.id}'] = []\n",
    "    \n",
    "# Initialize the penalty weight for the change in the cooperation output.\n",
    "for agent in agents:\n",
    "    # Initialize an empty previously optimal cooperation output for each agent.\n",
    "    agent.yT_pre = None\n",
    "    agent.MPC_sol = None\n",
    "    \n",
    "    agent.penalty_weight = 1e-4/T  # Set the weight of the penalty on the change in the cooperation output.\n",
    "    data['agents'][f'A{agent.id}']['penalty_weight'] = agent.penalty_weight\n",
    "\n",
    "# Build the closed-loop state evolution of each agent and save it as an attribute of the agent.\n",
    "for agent in agents:\n",
    "    agent.cl_x = [agent.current_state.copy()]\n",
    "    agent.cl_u = []\n",
    "    \n",
    "# Initialize a filestamp if continuous saving is activated.\n",
    "if save_data and save_interval_steps > 0:\n",
    "    filestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filestamp = f'{filestamp}_quicksave_{coop_task}'\n",
    "\n",
    "for t in range(0, max_sim_time+1):    \n",
    "    print(f\"{t}: -------------------------------------------------------------------------\")  # Print the time step:\n",
    "    if t == 0:\n",
    "        warm_start = aux.satellite_warm_start_at_t0(agents, N, T)\n",
    "    else:\n",
    "        warm_start = aux.compute_decentralized_following_warm_start_dynamic_cooperative_DMPC(agents, T, N, terminal_ingredients_type=terminal_ingredients_type)\n",
    "            \n",
    "    coop_kwargs['t'] = t\n",
    "    coop_kwargs['agents'] = agents\n",
    "    # Generate and solve the optimization problem for MPC for dynamic cooperation.\n",
    "    res = aux.solve_MPC_for_dynamic_cooperation_decentrally(sqp_max_iter, admm_max_iter, admm_penalty,\n",
    "            t, agents, N=N, T=T, feas_tol=1e-8, \n",
    "            warm_start=warm_start, \n",
    "            solver=solver,\n",
    "            terminal_ingredients_type=terminal_ingredients_type,\n",
    "            coop_task_builder=coop_task_builder, \n",
    "            coop_kwargs=coop_kwargs,\n",
    "            max_iter=max_iter,\n",
    "            verbose=2,\n",
    "            parallel=parallel)\n",
    "    print(f'Solved at time step {t} with f* = {float(res[\"J\"]):.5e}')\n",
    "    \n",
    "    data['sim_data']['tracking_cost'].append(res['tracking_cost'])\n",
    "    data['sim_data']['cooperative_cost'].append(res['cooperative_cost'])\n",
    "    data['sim_data']['change_cost'].append(res['change_cost']) \n",
    "    data['sim_data']['J'].append(res['J']) \n",
    "    \n",
    "    for agent in agents:\n",
    "        # Keep track of open-loop solutions, reshaped ready for plotting.\n",
    "        data['sim_data']['yT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_yT'].reshape(T, agent.output_dim).T)\n",
    "        data['sim_data']['xT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_xT'].reshape(T, agent.state_dim).T)\n",
    "        data['sim_data']['uT'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_uT'].reshape(T, agent.input_dim).T)\n",
    "        data['sim_data']['u'][f'A{agent.id}'].append(agent.MPC_sol[f'A{agent.id}_u'].reshape(N, agent.input_dim).T)\n",
    "        # The prediction starts with x(1|t), hence x(0|t) = x(t) needs to be prepended.\n",
    "        data['sim_data']['x'][f'A{agent.id}'].append(np.hstack([np.array(agent.current_state), agent.MPC_sol[f'A{agent.id}_x'].reshape(N, agent.state_dim).T]))\n",
    "\n",
    "        # Update the current state of the agents.\n",
    "        agent.current_state = agent.dynamics(x=agent.current_state, u=agent.MPC_sol[f'A{agent.id}_u'][0:agent.input_dim])['x+']\n",
    "        agent.cl_x.append(np.array(agent.current_state))  # Keep track of the current state.\n",
    "        agent.cl_u.append(np.array(agent.MPC_sol[f'A{agent.id}_u'][0:agent.input_dim]))  # Keep track of the current input.\n",
    "        \n",
    "        # Set the previously optimal trajectory:\n",
    "        agent.yT_pre = np.vstack([agent.MPC_sol[f'A{agent.id}_yT'][agent.output_dim :], agent.MPC_sol[f'A{agent.id}_yT'][0 : agent.output_dim]])\n",
    "        # For satellite agents, the second state (theta; angular position) wraps around 2pi and needs to be adjusted.\n",
    "        # Here, shifting is not enough, since the agent's state increments theta and does not consider the modulo behaviour.\n",
    "        # Hence, theta needs to be increased by 2pi when shifted.\n",
    "        if isinstance(agent, aux.Satellite) and agent.state_dim == 4:\n",
    "            agent.yT_pre[-2] = agent.yT_pre[-2] + 2*np.pi\n",
    "        \n",
    "    # Stop the simulation if the cost falls below a threshold.\n",
    "    if res['J'] <= cutoff_treshold:\n",
    "        print(f\"The value function has fallen below {cutoff_treshold} at time step {t}.\")\n",
    "        data['sim_data']['max_sim_time'] = t\n",
    "        break\n",
    "    # Stop the simulation if the cost has converged; i.e. the standard deviation over a window has fallen below a threshold.\n",
    "    if t > 10 and np.std(data['sim_data']['J'][t-10:t]) <= average_treshold:\n",
    "        print(f\"The standard deviation of the value function has fallen below {average_treshold} at time step {t}.\")\n",
    "        data['sim_data']['max_sim_time'] = t\n",
    "        break\n",
    "    # Save the data after each specified time step.\n",
    "    if save_data and t > 0 and save_interval_steps > 0 and t % save_interval_steps == 0:\n",
    "        aux.save_data(data, agents, filestamp)\n",
    "        \n",
    "    # In the constellation cooperative task, deorbit some satellites after some time.\n",
    "    if coop_task == 'constellation' and (t == deorbit_time or res['cooperative_cost'] <= 1e-9) and not deorbited_satellites:\n",
    "        # Save the data.\n",
    "        aux.save_data(data, agents, filestamp + f'_before_deorbit_at_t_{t}') \n",
    "        # Remove some agents from the agents list.\n",
    "        deorbited_satellites.append(agents.pop(1))\n",
    "        deorbited_satellites.append(agents.pop(2))\n",
    "        # Also update the neighbours.\n",
    "        agents[0].neighbours = [agents[1]]\n",
    "        for i, agent in enumerate(agents[1:-1]):\n",
    "            i += 1\n",
    "            agent.neighbours = [agents[i-1], agents[i+1]]\n",
    "        agents[-1].neighbours = [agents[-2]]\n",
    "        # Update the cooperative task.\n",
    "        coop_task_builder(**coop_kwargs)\n",
    "    \n",
    "# In the constellation task, add the deorbited satellites again to the agents list.\n",
    "if coop_task == 'constellation':\n",
    "    for sat in deorbited_satellites:\n",
    "        agents.append(sat)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Total runtime: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Colour palette\"\"\"\n",
    "colours = [\n",
    "    \"#0072B2\",  # blue\n",
    "    \"#D55E00\",  # orange\n",
    "    \"#009E73\",  # green\n",
    "    \"#CC79A7\",  # magenta\n",
    "    \"#56B4E9\",  # light blue\n",
    "    \"#E69F00\",  # yellow-orange\n",
    "    \"#B22222\",  # red\n",
    "    \"#6A3D9A\",  # purple\n",
    "    \"#117733\",  # teal green\n",
    "    \"#88CCEE\",  # cyan\n",
    "    \"#DDCC77\",  # muted yellow-orange\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transform data.\"\"\"\n",
    "# Transform the costs into numpy arrays.\n",
    "data['sim_data']['cooperative_cost'] = np.vstack(data['sim_data']['cooperative_cost']).flatten()\n",
    "data['sim_data']['tracking_cost'] = np.vstack(data['sim_data']['tracking_cost']).flatten()\n",
    "data['sim_data']['change_cost'] = np.vstack(data['sim_data']['change_cost']).flatten()\n",
    "data['sim_data']['J'] = np.vstack(data['sim_data']['J']).flatten()\n",
    "\n",
    "# Extract some parameters.\n",
    "max_sim_time = data['sim_data']['max_sim_time']\n",
    "\n",
    "# Transform the tracked closed-loop trajectories of each agent into a matrix.\n",
    "for agent in agents:\n",
    "    if type(agent.cl_x) == list:\n",
    "        agent.cl_x = np.hstack(agent.cl_x)\n",
    "        agent.cl_u = np.hstack(agent.cl_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save data\"\"\"\n",
    "if save_data:\n",
    "    aux.save_data(data, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Value function.\"\"\"\n",
    "# Plot from t1 to t2.\n",
    "t1 = 0\n",
    "t2 = max_sim_time+1\n",
    "\n",
    "# Select a feasible start time (the end time is controlled below).\n",
    "t1 = min(t1, max_sim_time+1)\n",
    "\n",
    "# Draw the evolution in state space:\n",
    "fig_V, ax_V = plt.subplots(figsize=(10, 6), num='state evolution')\n",
    "\n",
    "stop_time = data['sim_data']['cooperative_cost'][t1:t2].shape[0]\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['cooperative_cost'][t1:t2], label='cooperative', color=colours[0])\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['tracking_cost'][t1:t2], label='tracking', color=colours[1])\n",
    "ax_V.plot(range(max(t1,1), min(t2, stop_time)), data['sim_data']['change_cost'][max(t1,1):t2], label='change', color=colours[2])\n",
    "ax_V.plot(range(t1, min(t2, stop_time)), data['sim_data']['J'][t1:t2], '--', label='J', color=colours[3])\n",
    "\n",
    "ax_V.set_xlabel('time steps')\n",
    "ax_V.set_title('Value function over time')\n",
    "ax_V.grid()\n",
    "ax_V.legend()\n",
    "\n",
    "#ax_V.set_yscale('log')  # Set the y-axis to logarithmic scale.\n",
    "\n",
    "print(f\"\\nTotal runtime: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\\n\")\n",
    "\n",
    "print(f'Value function difference between the first and last time step: {data[\"sim_data\"][\"J\"][-1] - data[\"sim_data\"][\"J\"][0]}')\n",
    "print(f'Value function at start: {data[\"sim_data\"][\"J\"][0]:15.4e}')\n",
    "print(f'Value function at stop : {data[\"sim_data\"][\"J\"][-1]:15.4e}; diff: {data[\"sim_data\"][\"J\"][-1] - data[\"sim_data\"][\"J\"][0]:15.4e}')\n",
    "print(f'Cooperation cost at start : {data[\"sim_data\"][\"cooperative_cost\"][0]:15.4e}')\n",
    "print(f'Cooperation cost at stop : {data[\"sim_data\"][\"cooperative_cost\"][-1]:15.4e}; diff: {data[\"sim_data\"][\"cooperative_cost\"][-1] - data[\"sim_data\"][\"cooperative_cost\"][0]:15.4e}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2D position\"\"\"\n",
    "# Plot from t1 to t2.\n",
    "t1 = 0\n",
    "t2 = agent.cl_x.shape[1]-1\n",
    "step = 1\n",
    "\n",
    "# Select a feasible start time (the end time is controlled automatically).\n",
    "t1 = min(t1, max_sim_time+1)\n",
    "\n",
    "# Draw the evolution in state space:\n",
    "fig_cl, ax_cl = plt.subplots(figsize=(10, 10), num='state evolution')\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    cl_x = np.zeros(agent.cl_x.shape)\n",
    "    # For the constellation task, transform the polar coordinates into Cartesian coordinates.\n",
    "    if coop_task == 'constellation':\n",
    "        cl_x[0, t1 : t2+1:step] = (agent.cl_x[0, t1 : t2+1:step] + agent.r0) * np.cos(agent.cl_x[1, t1 : t2+1:step])\n",
    "        cl_x[1, t1 : t2+1:step] = (agent.cl_x[0, t1 : t2+1:step] + agent.r0) * np.sin(agent.cl_x[1, t1 : t2+1:step])\n",
    "    else:\n",
    "        cl_x = agent.cl_x\n",
    "    ax_cl.plot(cl_x[0, t1 : t2+1:step], cl_x[1,t1 : t2+1:step], color=colours[i], label=f'A{agent.id}_x', \n",
    "               #marker='o', markersize=2, \n",
    "               linewidth=1.5)\n",
    "    # Mark the initial state with a larger circle.\n",
    "    ax_cl.plot(cl_x[0,t1], cl_x[1,t1], color=colours[i], marker='o', markersize=6)\n",
    "    # Mark the final state with a cross.\n",
    "    ax_cl.plot(cl_x[0,t2], cl_x[1,t2], color=colours[i], marker='x', markersize=6)\n",
    "\n",
    "if coop_task == 'constellation':\n",
    "    xlabel=f'$x_1$ ({scaling})'\n",
    "    ylabel=f'$x_2$ ({scaling})'\n",
    "else:\n",
    "    xlabel='$x_1$'\n",
    "    ylabel='$x_2$'\n",
    "    \n",
    "if coop_task == 'constellation':\n",
    "    sf = data['MAS_parameters']['scaling_factor']\n",
    "    r0 = data['MAS_parameters']['r0']\n",
    "    r_max = (data['MAS_parameters']['r_max'] + r0)*sf\n",
    "    ax_cl.set_xlim(-r_max, r_max)\n",
    "    ax_cl.set_ylim(-r_max, r_max)\n",
    "    # Plot Earth\n",
    "    circle = patches.Circle((0, 0), 6371e3*sf, facecolor='#4169E1', fill=True, edgecolor='#4169E1', linewidth=2) \n",
    "    ax_cl.add_patch(circle)\n",
    "\n",
    "ax_cl.set_xlabel(xlabel)\n",
    "ax_cl.set_ylabel(ylabel)\n",
    "ax_cl.set_title(f'Closed-loop position from $t = {t1}$ to $t = {t2}$ with step {step}')\n",
    "ax_cl.grid()\n",
    "ax_cl.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
